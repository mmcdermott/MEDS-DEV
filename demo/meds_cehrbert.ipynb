{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9226e6d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Use a python 3.11 kernel cehrbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4c2dca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting meds_reader==0.1.9\n",
      "  Downloading meds_reader-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow>=9 (from meds_reader==0.1.9)\n",
      "  Downloading pyarrow-18.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting meds==0.3.3 (from meds_reader==0.1.9)\n",
      "  Using cached meds-0.3.3-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting numpy<2,>=1.16 (from meds_reader==0.1.9)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting pandas>=2.2 (from meds_reader==0.1.9)\n",
      "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting jsonschema>=4.0.0 (from meds==0.3.3->meds_reader==0.1.9)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from meds==0.3.3->meds_reader==0.1.9) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from pandas>=2.2->meds_reader==0.1.9) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=2.2->meds_reader==0.1.9)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=2.2->meds_reader==0.1.9)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=4.0.0->meds==0.3.3->meds_reader==0.1.9)\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.0.0->meds==0.3.3->meds_reader==0.1.9)\n",
      "  Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.0.0->meds==0.3.3->meds_reader==0.1.9)\n",
      "  Using cached referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=4.0.0->meds==0.3.3->meds_reader==0.1.9)\n",
      "  Downloading rpds_py-0.22.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: six>=1.5 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=2.2->meds_reader==0.1.9) (1.17.0)\n",
      "Downloading meds_reader-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached meds-0.3.3-py3-none-any.whl (12 kB)\n",
      "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Downloading pyarrow-18.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (40.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m143.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Using cached referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.22.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (381 kB)\n",
      "Installing collected packages: pytz, tzdata, rpds-py, pyarrow, numpy, attrs, referencing, pandas, jsonschema-specifications, jsonschema, meds, meds_reader\n",
      "Successfully installed attrs-24.2.0 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 meds-0.3.3 meds_reader-0.1.9 numpy-1.26.4 pandas-2.2.3 pyarrow-18.1.0 pytz-2024.2 referencing-0.35.1 rpds-py-0.22.3 tzdata-2024.2\n",
      "Requirement already satisfied: setuptools in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (75.1.0)\n",
      "Collecting cehrbert==1.3.1\n",
      "  Using cached cehrbert-1.3.1-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting dask==2024.1.1 (from cehrbert==1.3.1)\n",
      "  Using cached dask-2024.1.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting datasets==2.16.1 (from cehrbert==1.3.1)\n",
      "  Using cached datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting evaluate==0.4.1 (from cehrbert==1.3.1)\n",
      "  Using cached evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting fast-ml==3.68 (from cehrbert==1.3.1)\n",
      "  Using cached fast_ml-3.68-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting femr==0.2.0 (from cehrbert==1.3.1)\n",
      "  Using cached femr-0.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting Jinja2==3.1.3 (from cehrbert==1.3.1)\n",
      "  Using cached Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: meds==0.3.3 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (0.3.3)\n",
      "Requirement already satisfied: meds_reader==0.1.9 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (0.1.9)\n",
      "Collecting networkx==3.2.1 (from cehrbert==1.3.1)\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting numpy==1.24.3 (from cehrbert==1.3.1)\n",
      "  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting packaging==23.2 (from cehrbert==1.3.1)\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pandas==2.2.0 (from cehrbert==1.3.1)\n",
      "  Downloading pandas-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting peft==0.10.0 (from cehrbert==1.3.1)\n",
      "  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting Pillow==10.3.0 (from cehrbert==1.3.1)\n",
      "  Downloading pillow-10.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting pyarrow==15.0.0 (from cehrbert==1.3.1)\n",
      "  Downloading pyarrow-15.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pydantic==2.6.0 (from cehrbert==1.3.1)\n",
      "  Downloading pydantic-2.6.0-py3-none-any.whl.metadata (81 kB)\n",
      "Collecting python-dateutil==2.8.2 (from cehrbert==1.3.1)\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting PyYAML==6.0.1 (from cehrbert==1.3.1)\n",
      "  Downloading PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting scikit-learn==1.4.0 (from cehrbert==1.3.1)\n",
      "  Downloading scikit_learn-1.4.0-1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy==1.12.0 (from cehrbert==1.3.1)\n",
      "  Downloading scipy-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting tensorflow==2.15.0 (from cehrbert==1.3.1)\n",
      "  Downloading tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Collecting tensorflow-datasets==4.5.2 (from cehrbert==1.3.1)\n",
      "  Downloading tensorflow_datasets-4.5.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting tqdm==4.66.1 (from cehrbert==1.3.1)\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting torch==2.4.0 (from cehrbert==1.3.1)\n",
      "  Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting tokenizers==0.19.0 (from cehrbert==1.3.1)\n",
      "  Downloading tokenizers-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting transformers==4.40.0 (from cehrbert==1.3.1)\n",
      "  Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n",
      "Collecting accelerate==0.31.0 (from cehrbert==1.3.1)\n",
      "  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting Werkzeug==3.0.1 (from cehrbert==1.3.1)\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting wandb==0.17.8 (from cehrbert==1.3.1)\n",
      "  Downloading wandb-0.17.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting xgboost==2.0.3 (from cehrbert==1.3.1)\n",
      "  Downloading xgboost-2.0.3-py3-none-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting cehrbert_data==0.0.4 (from cehrbert==1.3.1)\n",
      "  Downloading cehrbert_data-0.0.4-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: psutil in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from accelerate==0.31.0->cehrbert==1.3.1) (6.1.0)\n",
      "Collecting huggingface-hub (from accelerate==0.31.0->cehrbert==1.3.1)\n",
      "  Using cached huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors>=0.3.1 (from accelerate==0.31.0->cehrbert==1.3.1)\n",
      "  Using cached safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting pyspark==3.1.2 (from cehrbert_data==0.0.4->cehrbert==1.3.1)\n",
      "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.4/212.4 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting click>=8.1 (from dask==2024.1.1->cehrbert==1.3.1)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting cloudpickle>=1.5.0 (from dask==2024.1.1->cehrbert==1.3.1)\n",
      "  Using cached cloudpickle-3.1.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting fsspec>=2021.09.0 (from dask==2024.1.1->cehrbert==1.3.1)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting partd>=1.2.0 (from dask==2024.1.1->cehrbert==1.3.1)\n",
      "  Downloading partd-1.4.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting toolz>=0.10.0 (from dask==2024.1.1->cehrbert==1.3.1)\n",
      "  Downloading toolz-1.0.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting importlib-metadata>=4.13.0 (from dask==2024.1.1->cehrbert==1.3.1)\n",
      "  Downloading importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting filelock (from datasets==2.16.1->cehrbert==1.3.1)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting pyarrow-hotfix (from datasets==2.16.1->cehrbert==1.3.1)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.16.1->cehrbert==1.3.1)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting requests>=2.19.0 (from datasets==2.16.1->cehrbert==1.3.1)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets==2.16.1->cehrbert==1.3.1)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets==2.16.1->cehrbert==1.3.1)\n",
      "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2021.09.0 (from dask==2024.1.1->cehrbert==1.3.1)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets==2.16.1->cehrbert==1.3.1)\n",
      "  Downloading aiohttp-3.11.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting responses<0.19 (from evaluate==0.4.1->cehrbert==1.3.1)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting zstandard>=0.18 (from femr==0.2.0->cehrbert==1.3.1)\n",
      "  Downloading zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting icecream==2.1.3 (from femr==0.2.0->cehrbert==1.3.1)\n",
      "  Downloading icecream-2.1.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting nptyping==2.4.1 (from femr==0.2.0->cehrbert==1.3.1)\n",
      "  Downloading nptyping-2.4.1-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting msgpack>=1.0.5 (from femr==0.2.0->cehrbert==1.3.1)\n",
      "  Downloading msgpack-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting MarkupSafe>=2.0 (from Jinja2==3.1.3->cehrbert==1.3.1)\n",
      "  Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: jsonschema>=4.0.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from meds==0.3.3->cehrbert==1.3.1) (4.23.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from meds==0.3.3->cehrbert==1.3.1) (4.12.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from pandas==2.2.0->cehrbert==1.3.1) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from pandas==2.2.0->cehrbert==1.3.1) (2024.2)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic==2.6.0->cehrbert==1.3.1)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.16.1 (from pydantic==2.6.0->cehrbert==1.3.1)\n",
      "  Downloading pydantic_core-2.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from python-dateutil==2.8.2->cehrbert==1.3.1) (1.17.0)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn==1.4.0->cehrbert==1.3.1)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn==1.4.0->cehrbert==1.3.1)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow==2.15.0->cehrbert==1.3.1)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow==2.15.0->cehrbert==1.3.1)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow==2.15.0->cehrbert==1.3.1)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow==2.15.0->cehrbert==1.3.1)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow==2.15.0->cehrbert==1.3.1)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=2.9.0 (from tensorflow==2.15.0->cehrbert==1.3.1)\n",
      "  Downloading h5py-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow==2.15.0->cehrbert==1.3.1)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0->cehrbert==1.3.1)\n",
      "  Downloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow==2.15.0->cehrbert==1.3.1)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.15.0->cehrbert==1.3.1)\n",
      "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from tensorflow==2.15.0->cehrbert==1.3.1) (75.1.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==2.15.0->cehrbert==1.3.1)\n",
      "  Using cached termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0->cehrbert==1.3.1)\n",
      "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow==2.15.0->cehrbert==1.3.1)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow==2.15.0->cehrbert==1.3.1)\n",
      "  Downloading grpcio-1.68.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0->cehrbert==1.3.1)\n",
      "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0->cehrbert==1.3.1)\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0->cehrbert==1.3.1)\n",
      "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting promise (from tensorflow-datasets==4.5.2->cehrbert==1.3.1)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorflow-metadata (from tensorflow-datasets==4.5.2->cehrbert==1.3.1)\n",
      "  Downloading tensorflow_metadata-1.16.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting sympy (from torch==2.4.0->cehrbert==1.3.1)\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0->cehrbert==1.3.1)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0->cehrbert==1.3.1)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0->cehrbert==1.3.1)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0->cehrbert==1.3.1)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0->cehrbert==1.3.1)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0->cehrbert==1.3.1)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0->cehrbert==1.3.1)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0->cehrbert==1.3.1)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0->cehrbert==1.3.1)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0->cehrbert==1.3.1)\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0->cehrbert==1.3.1)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch==2.4.0->cehrbert==1.3.1)\n",
      "  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.40.0->cehrbert==1.3.1)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb==0.17.8->cehrbert==1.3.1)\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb==0.17.8->cehrbert==1.3.1)\n",
      "  Using cached GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from wandb==0.17.8->cehrbert==1.3.1) (4.3.6)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb==0.17.8->cehrbert==1.3.1)\n",
      "  Using cached sentry_sdk-2.19.2-py2.py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting setproctitle (from wandb==0.17.8->cehrbert==1.3.1)\n",
      "  Downloading setproctitle-1.3.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting colorama>=0.3.9 (from icecream==2.1.3->femr==0.2.0->cehrbert==1.3.1)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: pygments>=2.2.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from icecream==2.1.3->femr==0.2.0->cehrbert==1.3.1) (2.18.0)\n",
      "Requirement already satisfied: executing>=0.3.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from icecream==2.1.3->femr==0.2.0->cehrbert==1.3.1) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.0.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from icecream==2.1.3->femr==0.2.0->cehrbert==1.3.1) (3.0.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->cehrbert==1.3.1)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting py4j==0.10.9 (from pyspark==3.1.2->cehrbert_data==0.0.4->cehrbert==1.3.1)\n",
      "  Downloading py4j-0.10.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow==2.15.0->cehrbert==1.3.1) (0.44.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets==2.16.1->cehrbert==1.3.1)\n",
      "  Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==2.16.1->cehrbert==1.3.1)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from aiohttp->datasets==2.16.1->cehrbert==1.3.1) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==2.16.1->cehrbert==1.3.1)\n",
      "  Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.16.1->cehrbert==1.3.1)\n",
      "  Downloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets==2.16.1->cehrbert==1.3.1)\n",
      "  Downloading propcache-0.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets==2.16.1->cehrbert==1.3.1)\n",
      "  Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb==0.17.8->cehrbert==1.3.1)\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata>=4.13.0->dask==2024.1.1->cehrbert==1.3.1)\n",
      "  Downloading zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from jsonschema>=4.0.0->meds==0.3.3->cehrbert==1.3.1) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from jsonschema>=4.0.0->meds==0.3.3->cehrbert==1.3.1) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from jsonschema>=4.0.0->meds==0.3.3->cehrbert==1.3.1) (0.22.3)\n",
      "Collecting locket (from partd>=1.2.0->dask==2024.1.1->cehrbert==1.3.1)\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.19.0->datasets==2.16.1->cehrbert==1.3.1)\n",
      "  Using cached charset_normalizer-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.19.0->datasets==2.16.1->cehrbert==1.3.1)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.19.0->datasets==2.16.1->cehrbert==1.3.1)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.19.0->datasets==2.16.1->cehrbert==1.3.1)\n",
      "  Using cached certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0->cehrbert==1.3.1)\n",
      "  Downloading google_auth-2.37.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0->cehrbert==1.3.1)\n",
      "  Downloading google_auth_oauthlib-1.2.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0->cehrbert==1.3.1)\n",
      "  Using cached Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0->cehrbert==1.3.1)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.16.1->cehrbert==1.3.1)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.4.0->cehrbert==1.3.1)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.56.4 (from tensorflow-metadata->tensorflow-datasets==4.5.2->cehrbert==1.3.1)\n",
      "  Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb==0.17.8->cehrbert==1.3.1)\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0->cehrbert==1.3.1)\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0->cehrbert==1.3.1)\n",
      "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0->cehrbert==1.3.1)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0->cehrbert==1.3.1)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0->cehrbert==1.3.1)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0->cehrbert==1.3.1)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading cehrbert-1.3.1-py3-none-any.whl (139 kB)\n",
      "Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
      "Downloading cehrbert_data-0.0.4-py3-none-any.whl (77 kB)\n",
      "Downloading dask-2024.1.1-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
      "Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "Downloading fast_ml-3.68-py3-none-any.whl (42 kB)\n",
      "Downloading femr-0.2.0-py3-none-any.whl (84 kB)\n",
      "Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m134.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m143.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Downloading pandas-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m147.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
      "Downloading pillow-10.3.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m135.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-15.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (38.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.6.0-py3-none-any.whl (394 kB)\n",
      "Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Downloading PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.7/757.7 kB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.4.0-1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m134.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.3/475.3 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_datasets-4.5.2-py3-none-any.whl (4.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl (797.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.3/797.3 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Downloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m156.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wandb-0.17.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "Downloading xgboost-2.0.3-py3-none-manylinux2014_x86_64.whl (297.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.1/297.1 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading icecream-2.1.3-py2.py3-none-any.whl (8.4 kB)\n",
      "Downloading nptyping-2.4.1-py3-none-any.whl (36 kB)\n",
      "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Downloading pydantic_core-2.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m111.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m117.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "Downloading aiohttp-3.11.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m131.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.68.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m209.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m139.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n",
      "Downloading importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m133.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m201.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Downloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m125.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading msgpack-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (403 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading partd-1.4.2-py3-none-any.whl (18 kB)\n",
      "Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Using cached safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Using cached sentry_sdk-2.19.2-py2.py3-none-any.whl (322 kB)\n",
      "Downloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m166.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading toolz-1.0.0-py3-none-any.whl (56 kB)\n",
      "Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
      "Downloading zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m165.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading setproctitle-1.3.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
      "Using cached sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "Downloading tensorflow_metadata-1.16.1-py3-none-any.whl (28 kB)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n",
      "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Downloading google_auth-2.37.0-py2.py3-none-any.whl (209 kB)\n",
      "Downloading google_auth_oauthlib-1.2.1-py2.py3-none-any.whl (24 kB)\n",
      "Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "Downloading propcache-0.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n",
      "Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m226.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Building wheels for collected packages: pyspark, promise\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880746 sha256=cb39c2c9d3ea7a37b23b68fe02520cffab2313593b1757452eb1b95f77a305c7\n",
      "  Stored in directory: /storage/nassim/.cache/pip/wheels/9c/3c/bc/93eb7c1c3c6438508389e26e46dfe3ffa238d163d20a44f9de\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21483 sha256=bca34bdad6c19dccb64518cda5223a0e4ed34500696fc418ab668e267ed2e3c4\n",
      "  Stored in directory: /storage/nassim/.cache/pip/wheels/90/74/b1/9b54c896b8d9409e9268329d4d45ede8a8040abe91c8879932\n",
      "Successfully built pyspark promise\n",
      "Installing collected packages: py4j, mpmath, libclang, flatbuffers, zstandard, zipp, xxhash, wrapt, urllib3, tqdm, toolz, threadpoolctl, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, sympy, smmap, setproctitle, safetensors, regex, PyYAML, python-dateutil, pyspark, pydantic-core, pyasn1, pyarrow-hotfix, protobuf, propcache, promise, Pillow, packaging, opt-einsum, oauthlib, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, msgpack, MarkupSafe, markdown, locket, keras, joblib, idna, grpcio, google-pasta, gast, fsspec, frozenlist, filelock, fast-ml, docker-pycreds, dill, colorama, cloudpickle, click, charset-normalizer, certifi, cachetools, astunparse, annotated-types, aiohappyeyeballs, absl-py, yarl, Werkzeug, triton, sentry-sdk, scipy, rsa, requests, pydantic, pyasn1-modules, pyarrow, partd, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nptyping, multiprocess, ml-dtypes, Jinja2, importlib-metadata, icecream, h5py, googleapis-common-protos, gitdb, aiosignal, xgboost, tensorflow-metadata, scikit-learn, responses, requests-oauthlib, nvidia-cusolver-cu12, huggingface-hub, google-auth, gitpython, dask, cehrbert_data, aiohttp, wandb, torch, tokenizers, tensorflow-datasets, google-auth-oauthlib, femr, transformers, tensorboard, datasets, accelerate, tensorflow, peft, evaluate, cehrbert\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.2\n",
      "    Uninstalling packaging-24.2:\n",
      "      Successfully uninstalled packaging-24.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 18.1.0\n",
      "    Uninstalling pyarrow-18.1.0:\n",
      "      Successfully uninstalled pyarrow-18.1.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.3\n",
      "    Uninstalling pandas-2.2.3:\n",
      "      Successfully uninstalled pandas-2.2.3\n",
      "Successfully installed Jinja2-3.1.3 MarkupSafe-3.0.2 Pillow-10.3.0 PyYAML-6.0.1 Werkzeug-3.0.1 absl-py-2.1.0 accelerate-0.31.0 aiohappyeyeballs-2.4.4 aiohttp-3.11.10 aiosignal-1.3.2 annotated-types-0.7.0 astunparse-1.6.3 cachetools-5.5.0 cehrbert-1.3.1 cehrbert_data-0.0.4 certifi-2024.12.14 charset-normalizer-3.4.0 click-8.1.7 cloudpickle-3.1.0 colorama-0.4.6 dask-2024.1.1 datasets-2.16.1 dill-0.3.7 docker-pycreds-0.4.0 evaluate-0.4.1 fast-ml-3.68 femr-0.2.0 filelock-3.16.1 flatbuffers-24.3.25 frozenlist-1.5.0 fsspec-2023.10.0 gast-0.6.0 gitdb-4.0.11 gitpython-3.1.43 google-auth-2.37.0 google-auth-oauthlib-1.2.1 google-pasta-0.2.0 googleapis-common-protos-1.66.0 grpcio-1.68.1 h5py-3.12.1 huggingface-hub-0.26.5 icecream-2.1.3 idna-3.10 importlib-metadata-8.5.0 joblib-1.4.2 keras-2.15.0 libclang-18.1.1 locket-1.0.0 markdown-3.7 ml-dtypes-0.2.0 mpmath-1.3.0 msgpack-1.1.0 multidict-6.1.0 multiprocess-0.70.15 networkx-3.2.1 nptyping-2.4.1 numpy-1.24.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.1.105 oauthlib-3.2.2 opt-einsum-3.4.0 packaging-23.2 pandas-2.2.0 partd-1.4.2 peft-0.10.0 promise-2.3 propcache-0.2.1 protobuf-4.25.5 py4j-0.10.9 pyarrow-15.0.0 pyarrow-hotfix-0.6 pyasn1-0.6.1 pyasn1-modules-0.4.1 pydantic-2.6.0 pydantic-core-2.16.1 pyspark-3.1.2 python-dateutil-2.8.2 regex-2024.11.6 requests-2.32.3 requests-oauthlib-2.0.0 responses-0.18.0 rsa-4.9 safetensors-0.4.5 scikit-learn-1.4.0 scipy-1.12.0 sentry-sdk-2.19.2 setproctitle-1.3.4 smmap-5.0.1 sympy-1.13.3 tensorboard-2.15.2 tensorboard-data-server-0.7.2 tensorflow-2.15.0 tensorflow-datasets-4.5.2 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.37.1 tensorflow-metadata-1.16.1 termcolor-2.5.0 threadpoolctl-3.5.0 tokenizers-0.19.0 toolz-1.0.0 torch-2.4.0 tqdm-4.66.1 transformers-4.40.0 triton-3.0.0 urllib3-2.2.3 wandb-0.17.8 wrapt-1.14.1 xgboost-2.0.3 xxhash-3.5.0 yarl-1.18.3 zipp-3.21.0 zstandard-0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install meds_reader==0.1.9\n",
    "!pip install setuptools\n",
    "!pip install cehrbert==1.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff2f8638",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Download E-ICU demo\n",
    "import os\n",
    "from pathlib import Path\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "ROOT_DIR=f\"{notebook_dir}/work_dir/mimiciv_demo/\"\n",
    "# ROOT_DIR=f\"{notebook_dir}/work_dir/eicu_demo/\"\n",
    "Path(ROOT_DIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEDS_DIR = ROOT_DIR + \"/meds/\"\n",
    "MEDS_READER_DIR = ROOT_DIR + \"/meds_reader/\"\n",
    "TASK_DIR = MEDS_DIR + \"/task_labels\"\n",
    "TASK_NAME=\"mortality/in_icu/first_24h\"\n",
    "# TASK_NAME=\"los_in_hospital_first_48h\"\n",
    "OUTPUT_PRETRAIN_MODEL_DIR= ROOT_DIR + \"/output/cehrbert/train/\"\n",
    "# TODO this variable has an identical name?\n",
    "OUTPUT_FINETUNE_MODEL_DIR= ROOT_DIR + \"/output/cehrbert/finetuned/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e4ac93",
   "metadata": {},
   "source": [
    "Run meds_reader on the MEDS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6010330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!meds_reader_convert $MEDS_DIR $MEDS_READER_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26f1edfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {ROOT_DIR}/output/cehrbert/\n",
    "!mkdir -p {ROOT_DIR}/output/cehrbert_dataset_prepared/\n",
    "!mkdir -p {ROOT_DIR}/output/cehrbert_finetuned/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17c4af8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'cehrbert'...\n",
      "remote: Enumerating objects: 8757, done.\u001b[K\n",
      "remote: Counting objects: 100% (1428/1428), done.\u001b[K\n",
      "remote: Compressing objects: 100% (604/604), done.\u001b[K\n",
      "remote: Total 8757 (delta 934), reused 943 (delta 767), pack-reused 7329 (from 1)\u001b[K\n",
      "Receiving objects: 100% (8757/8757), 14.23 MiB | 25.66 MiB/s, done.\n",
      "Resolving deltas: 100% (6174/6174), done.\n",
      "error: pathspec 'fix/meds_evaluation' did not match any file(s) known to git\n",
      "Processing /storage/nassim/projects/MEDS-DEV/demo/work_dir/mimiciv_demo/github_repo/cehrbert\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: dask==2024.1.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (2024.1.1)\n",
      "Requirement already satisfied: datasets==2.16.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (2.16.1)\n",
      "Requirement already satisfied: evaluate==0.4.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (0.4.1)\n",
      "Requirement already satisfied: fast-ml==3.68 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (3.68)\n",
      "Requirement already satisfied: femr==0.2.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (0.2.0)\n",
      "Requirement already satisfied: Jinja2==3.1.3 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (3.1.3)\n",
      "Requirement already satisfied: meds==0.3.3 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (0.3.3)\n",
      "Requirement already satisfied: meds_reader==0.1.9 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (0.1.9)\n",
      "Requirement already satisfied: networkx==3.2.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (3.2.1)\n",
      "Requirement already satisfied: numpy==1.24.3 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (1.24.3)\n",
      "Requirement already satisfied: packaging==23.2 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (23.2)\n",
      "Requirement already satisfied: pandas==2.2.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (2.2.0)\n",
      "Requirement already satisfied: peft==0.10.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (0.10.0)\n",
      "Requirement already satisfied: Pillow==10.3.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (10.3.0)\n",
      "Requirement already satisfied: pyarrow==15.0.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (15.0.0)\n",
      "Requirement already satisfied: pydantic==2.6.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (2.6.0)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (2.8.2)\n",
      "Requirement already satisfied: PyYAML==6.0.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (6.0.1)\n",
      "Requirement already satisfied: scikit-learn==1.4.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (1.4.0)\n",
      "Requirement already satisfied: scipy==1.12.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (1.12.0)\n",
      "Requirement already satisfied: tensorflow==2.15.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (2.15.0)\n",
      "Requirement already satisfied: tensorflow-datasets==4.5.2 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (4.5.2)\n",
      "Requirement already satisfied: tqdm==4.66.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (4.66.1)\n",
      "Requirement already satisfied: torch==2.4.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (2.4.0)\n",
      "Requirement already satisfied: tokenizers==0.19.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (0.19.0)\n",
      "Requirement already satisfied: transformers==4.40.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (4.40.0)\n",
      "Requirement already satisfied: accelerate==0.31.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (0.31.0)\n",
      "Requirement already satisfied: Werkzeug==3.0.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (3.0.1)\n",
      "Requirement already satisfied: wandb==0.17.8 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (0.17.8)\n",
      "Requirement already satisfied: xgboost==2.0.3 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (2.0.3)\n",
      "Requirement already satisfied: cehrbert_data==0.0.4 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert==1.3.1) (0.0.4)\n",
      "Requirement already satisfied: psutil in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from accelerate==0.31.0->cehrbert==1.3.1) (6.1.0)\n",
      "Requirement already satisfied: huggingface-hub in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from accelerate==0.31.0->cehrbert==1.3.1) (0.26.5)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from accelerate==0.31.0->cehrbert==1.3.1) (0.4.5)\n",
      "Requirement already satisfied: pyspark==3.1.2 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from cehrbert_data==0.0.4->cehrbert==1.3.1) (3.1.2)\n",
      "Requirement already satisfied: click>=8.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from dask==2024.1.1->cehrbert==1.3.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from dask==2024.1.1->cehrbert==1.3.1) (3.1.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from dask==2024.1.1->cehrbert==1.3.1) (2023.10.0)\n",
      "Requirement already satisfied: partd>=1.2.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from dask==2024.1.1->cehrbert==1.3.1) (1.4.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from dask==2024.1.1->cehrbert==1.3.1) (1.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from dask==2024.1.1->cehrbert==1.3.1) (8.5.0)\n",
      "Requirement already satisfied: filelock in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from datasets==2.16.1->cehrbert==1.3.1) (3.16.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from datasets==2.16.1->cehrbert==1.3.1) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from datasets==2.16.1->cehrbert==1.3.1) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.19.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from datasets==2.16.1->cehrbert==1.3.1) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from datasets==2.16.1->cehrbert==1.3.1) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from datasets==2.16.1->cehrbert==1.3.1) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from datasets==2.16.1->cehrbert==1.3.1) (3.11.10)\n",
      "Requirement already satisfied: responses<0.19 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from evaluate==0.4.1->cehrbert==1.3.1) (0.18.0)\n",
      "Requirement already satisfied: zstandard>=0.18 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from femr==0.2.0->cehrbert==1.3.1) (0.23.0)\n",
      "Requirement already satisfied: icecream==2.1.3 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from femr==0.2.0->cehrbert==1.3.1) (2.1.3)\n",
      "Requirement already satisfied: nptyping==2.4.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from femr==0.2.0->cehrbert==1.3.1) (2.4.1)\n",
      "Requirement already satisfied: msgpack>=1.0.5 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from femr==0.2.0->cehrbert==1.3.1) (1.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from Jinja2==3.1.3->cehrbert==1.3.1) (3.0.2)\n",
      "Requirement already satisfied: jsonschema>=4.0.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from meds==0.3.3->cehrbert==1.3.1) (4.23.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from meds==0.3.3->cehrbert==1.3.1) (4.12.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from pandas==2.2.0->cehrbert==1.3.1) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from pandas==2.2.0->cehrbert==1.3.1) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from pydantic==2.6.0->cehrbert==1.3.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from pydantic==2.6.0->cehrbert==1.3.1) (2.16.1)\n",
      "Requirement already satisfied: six>=1.5 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from python-dateutil==2.8.2->cehrbert==1.3.1) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from scikit-learn==1.4.0->cehrbert==1.3.1) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from scikit-learn==1.4.0->cehrbert==1.3.1) (3.5.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from tensorflow==2.15.0->cehrbert==1.3.1) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from tensorflow==2.15.0->cehrbert==1.3.1) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from tensorflow==2.15.0->cehrbert==1.3.1) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from tensorflow==2.15.0->cehrbert==1.3.1) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from tensorflow==2.15.0->cehrbert==1.3.1) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from tensorflow==2.15.0->cehrbert==1.3.1) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from tensorflow==2.15.0->cehrbert==1.3.1) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from tensorflow==2.15.0->cehrbert==1.3.1) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from tensorflow==2.15.0->cehrbert==1.3.1) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from tensorflow==2.15.0->cehrbert==1.3.1) (4.25.5)\n",
      "Requirement already satisfied: setuptools in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from tensorflow==2.15.0->cehrbert==1.3.1) (75.1.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from tensorflow==2.15.0->cehrbert==1.3.1) (2.5.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from tensorflow==2.15.0->cehrbert==1.3.1) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from tensorflow==2.15.0->cehrbert==1.3.1) (0.37.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from tensorflow==2.15.0->cehrbert==1.3.1) (1.68.1)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from tensorflow==2.15.0->cehrbert==1.3.1) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from tensorflow==2.15.0->cehrbert==1.3.1) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from tensorflow==2.15.0->cehrbert==1.3.1) (2.15.0)\n",
      "Requirement already satisfied: promise in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from tensorflow-datasets==4.5.2->cehrbert==1.3.1) (2.3)\n",
      "Requirement already satisfied: tensorflow-metadata in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from tensorflow-datasets==4.5.2->cehrbert==1.3.1) (1.16.1)\n",
      "Requirement already satisfied: sympy in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from torch==2.4.0->cehrbert==1.3.1) (1.13.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from torch==2.4.0->cehrbert==1.3.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from torch==2.4.0->cehrbert==1.3.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from torch==2.4.0->cehrbert==1.3.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from torch==2.4.0->cehrbert==1.3.1) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from torch==2.4.0->cehrbert==1.3.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from torch==2.4.0->cehrbert==1.3.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from torch==2.4.0->cehrbert==1.3.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from torch==2.4.0->cehrbert==1.3.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from torch==2.4.0->cehrbert==1.3.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from torch==2.4.0->cehrbert==1.3.1) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from torch==2.4.0->cehrbert==1.3.1) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from torch==2.4.0->cehrbert==1.3.1) (3.0.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from transformers==4.40.0->cehrbert==1.3.1) (2024.11.6)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from wandb==0.17.8->cehrbert==1.3.1) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from wandb==0.17.8->cehrbert==1.3.1) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from wandb==0.17.8->cehrbert==1.3.1) (4.3.6)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from wandb==0.17.8->cehrbert==1.3.1) (2.19.2)\n",
      "Requirement already satisfied: setproctitle in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from wandb==0.17.8->cehrbert==1.3.1) (1.3.4)\n",
      "Requirement already satisfied: colorama>=0.3.9 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from icecream==2.1.3->femr==0.2.0->cehrbert==1.3.1) (0.4.6)\n",
      "Requirement already satisfied: pygments>=2.2.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from icecream==2.1.3->femr==0.2.0->cehrbert==1.3.1) (2.18.0)\n",
      "Requirement already satisfied: executing>=0.3.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from icecream==2.1.3->femr==0.2.0->cehrbert==1.3.1) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.0.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from icecream==2.1.3->femr==0.2.0->cehrbert==1.3.1) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->cehrbert==1.3.1) (12.6.85)\n",
      "Requirement already satisfied: py4j==0.10.9 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from pyspark==3.1.2->cehrbert_data==0.0.4->cehrbert==1.3.1) (0.10.9)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow==2.15.0->cehrbert==1.3.1) (0.44.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from aiohttp->datasets==2.16.1->cehrbert==1.3.1) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from aiohttp->datasets==2.16.1->cehrbert==1.3.1) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from aiohttp->datasets==2.16.1->cehrbert==1.3.1) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from aiohttp->datasets==2.16.1->cehrbert==1.3.1) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from aiohttp->datasets==2.16.1->cehrbert==1.3.1) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from aiohttp->datasets==2.16.1->cehrbert==1.3.1) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from aiohttp->datasets==2.16.1->cehrbert==1.3.1) (1.18.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb==0.17.8->cehrbert==1.3.1) (4.0.11)\n",
      "Requirement already satisfied: zipp>=3.20 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from importlib-metadata>=4.13.0->dask==2024.1.1->cehrbert==1.3.1) (3.21.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from jsonschema>=4.0.0->meds==0.3.3->cehrbert==1.3.1) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from jsonschema>=4.0.0->meds==0.3.3->cehrbert==1.3.1) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from jsonschema>=4.0.0->meds==0.3.3->cehrbert==1.3.1) (0.22.3)\n",
      "Requirement already satisfied: locket in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from partd>=1.2.0->dask==2024.1.1->cehrbert==1.3.1) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.16.1->cehrbert==1.3.1) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.16.1->cehrbert==1.3.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.16.1->cehrbert==1.3.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.16.1->cehrbert==1.3.1) (2024.12.14)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0->cehrbert==1.3.1) (2.37.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0->cehrbert==1.3.1) (1.2.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0->cehrbert==1.3.1) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0->cehrbert==1.3.1) (0.7.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from sympy->torch==2.4.0->cehrbert==1.3.1) (1.3.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from tensorflow-metadata->tensorflow-datasets==4.5.2->cehrbert==1.3.1) (1.66.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb==0.17.8->cehrbert==1.3.1) (5.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0->cehrbert==1.3.1) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0->cehrbert==1.3.1) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0->cehrbert==1.3.1) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0->cehrbert==1.3.1) (2.0.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0->cehrbert==1.3.1) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0->cehrbert==1.3.1) (3.2.2)\n",
      "Building wheels for collected packages: cehrbert\n",
      "  Building wheel for cehrbert (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cehrbert: filename=cehrbert-1.3.1-py3-none-any.whl size=139740 sha256=b67dbe857d4ecf2d4c1c7b2fad5ece256e911d33abef142535a1210274be4205\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-j1feuvmd/wheels/0f/e9/2b/6ce226fd4943719e4d11e237506934d50e642553b2941d21c5\n",
      "Successfully built cehrbert\n",
      "Installing collected packages: cehrbert\n",
      "  Attempting uninstall: cehrbert\n",
      "    Found existing installation: cehrbert 1.3.1\n",
      "    Uninstalling cehrbert-1.3.1:\n",
      "      Successfully uninstalled cehrbert-1.3.1\n",
      "Successfully installed cehrbert-1.3.1\n"
     ]
    }
   ],
   "source": [
    "!mkdir {ROOT_DIR}/github_repo;cd {ROOT_DIR}/github_repo;git clone https://github.com/cumc-dbmi/cehrbert.git;cd cehrbert;git checkout fix/meds_evaluation;pip install .;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e10d2",
   "metadata": {},
   "source": [
    "Create the cehrbert pretraining configuration yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f033a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "cehrbert_pretrain_config = f\"\"\"\n",
    "#Model arguments\n",
    "model_name_or_path: \"{ROOT_DIR}/output/cehrbert/\"\n",
    "tokenizer_name_or_path: \"{ROOT_DIR}/output/cehrbert/\"\n",
    "num_hidden_layers: 6\n",
    "max_position_embeddings: 1024\n",
    "hidden_size: 768\n",
    "vocab_size: 100000\n",
    "min_frequency: 50\n",
    "include_value_prediction: false # additional CEHR-BERT learning objective\n",
    "\n",
    "#Data arguments\n",
    "data_folder: \"{ROOT_DIR}/meds_reader/\"\n",
    "dataset_prepared_path: \"{ROOT_DIR}/output/cehrbert_dataset_prepared/\"\n",
    "\n",
    "# Below is a list of Med-to-CehrBert related arguments\n",
    "preprocessing_num_workers: 2\n",
    "preprocessing_batch_size: 128\n",
    "# if is_data_in_med is false, it assumes the data is in the cehrbert format\n",
    "is_data_in_meds: true\n",
    "att_function_type: \"cehr_bert\"\n",
    "inpatient_att_function_type: \"mix\"\n",
    "include_auxiliary_token: true\n",
    "include_demographic_prompt: false\n",
    "# if the data is in the meds format, the validation split will be omitted\n",
    "# as the meds already provide train/tuning/held_out splits\n",
    "validation_split_percentage: 0.05\n",
    "\n",
    "# Huggingface Arguments\n",
    "dataloader_num_workers: 2\n",
    "dataloader_prefetch_factor: 2\n",
    "\n",
    "overwrite_output_dir: false\n",
    "resume_from_checkpoint: # automatically infer the latest checkpoint from the output folder\n",
    "seed: 42\n",
    "\n",
    "output_dir: \"{ROOT_DIR}/output/cehrbert/\"\n",
    "evaluation_strategy: \"epoch\"\n",
    "save_strategy: \"epoch\"\n",
    "eval_accumulation_steps: 10\n",
    "\n",
    "learning_rate: 0.00005\n",
    "per_device_train_batch_size: 8\n",
    "per_device_eval_batch_size: 8\n",
    "gradient_accumulation_steps: 2\n",
    "\n",
    "num_train_epochs: 50 # for large datasets, 5-10 epochs should suffice\n",
    "warmup_steps: 10\n",
    "weight_decay: 0.01\n",
    "logging_dir: \"./logs\"\n",
    "logging_steps: 10\n",
    "\n",
    "save_total_limit:\n",
    "load_best_model_at_end: true\n",
    "metric_for_best_model: \"eval_loss\"\n",
    "greater_is_better: false\n",
    "\n",
    "report_to: \"none\"\n",
    "\"\"\"\n",
    "PRETRAIN_CONFIG_FP = ROOT_DIR + \"/output/cehrbert/cehrbert_pretrain_config.yaml\"\n",
    "with open(PRETRAIN_CONFIG_FP, 'w') as f:\n",
    "    f.write(cehrbert_pretrain_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20844f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-15 03:56:01.498490: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-15 03:56:01.498535: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-15 03:56:01.499647: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-15 03:56:01.506092: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-15 03:56:02.465497: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Directory /storage/nassim/projects/MEDS-DEV/demo/work_dir/mimiciv_demo//output/cehrbert_dataset_prepared/meds_reader_meds_extension not found\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/cehrbert/runners/hf_cehrbert_pretrain_runner.py\", line 183, in main\n",
      "    dataset = load_from_disk(meds_extension_path)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/datasets/load.py\", line 2630, in load_from_disk\n",
      "    raise FileNotFoundError(f\"Directory {dataset_path} not found\")\n",
      "FileNotFoundError: Directory /storage/nassim/projects/MEDS-DEV/demo/work_dir/mimiciv_demo//output/cehrbert_dataset_prepared/meds_reader_meds_extension not found\n",
      "Generating train split: 50 examples [00:03, 14.66 examples/s]\n",
      "Map (num_proc=2): 100%|██████████████████| 50/50 [00:05<00:00,  9.41 examples/s]\n",
      "Generating train split: 25 examples [00:01, 13.20 examples/s]\n",
      "Map (num_proc=2): 100%|██████████████████| 25/25 [00:02<00:00,  8.92 examples/s]\n",
      "Generating train split: 25 examples [00:00, 29.81 examples/s]\n",
      "Map (num_proc=2): 100%|██████████████████| 25/25 [00:01<00:00, 19.80 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██| 50/50 [00:00<00:00, 277.76 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██| 25/25 [00:00<00:00, 284.20 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██| 25/25 [00:00<00:00, 619.16 examples/s]\n",
      "Failed to load the tokenizer from /storage/nassim/projects/MEDS-DEV/demo/work_dir/mimiciv_demo//output/cehrbert/ with the error \n",
      "/storage/nassim/projects/MEDS-DEV/demo/work_dir/mimiciv_demo//output/cehrbert/ does not appear to have a file named tokenizer.json. Checkout 'https://huggingface.co//storage/nassim/projects/MEDS-DEV/demo/work_dir/mimiciv_demo//output/cehrbert//tree/None' for available files.\n",
      "Tried to create the tokenizer, however the dataset is not provided.\n",
      "Map (num_proc=2): 100%|█████████████████| 50/50 [00:00<00:00, 104.75 examples/s]\n",
      "Map (num_proc=2): 100%|██████████████████| 50/50 [00:01<00:00, 38.83 examples/s]   0\n",
      "Aggregating the lab statistics: 100%|█████████████| 2/2 [00:00<00:00,  7.44it/s]\n",
      "Filter (num_proc=2): 100%|██████████████| 50/50 [00:00<00:00, 319.20 examples/s]\n",
      "Filter (num_proc=2): 100%|██████████████| 25/25 [00:00<00:00, 168.34 examples/s]\n",
      "Filter (num_proc=2): 100%|██████████████| 25/25 [00:00<00:00, 162.55 examples/s]\n",
      "Map (num_proc=2): 100%|██████████████████| 50/50 [00:04<00:00, 11.91 examples/s]\n",
      "Map (num_proc=2): 100%|██████████████████| 25/25 [00:02<00:00, 11.84 examples/s]\n",
      "Map (num_proc=2): 100%|██████████████████| 25/25 [00:01<00:00, 20.68 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██| 50/50 [00:00<00:00, 314.80 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██| 25/25 [00:00<00:00, 355.17 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██| 25/25 [00:00<00:00, 774.69 examples/s]\n",
      "Filter (num_proc=2): 100%|██████████████| 50/50 [00:00<00:00, 319.76 examples/s]\n",
      "Filter (num_proc=2): 100%|██████████████| 25/25 [00:00<00:00, 162.83 examples/s]\n",
      "Filter (num_proc=2): 100%|██████████████| 25/25 [00:00<00:00, 162.27 examples/s]\n",
      "/storage/nassim/projects/MEDS-DEV/demo/work_dir/mimiciv_demo//output/cehrbert/ does not appear to have a file named config.json. Checkout 'https://huggingface.co//storage/nassim/projects/MEDS-DEV/demo/work_dir/mimiciv_demo//output/cehrbert//tree/None' for available files.\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "  2%|▉                                           | 1/50 [00:10<08:35, 10.52s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 7.287792205810547, 'eval_runtime': 3.1453, 'eval_samples_per_second': 7.948, 'eval_steps_per_second': 0.318, 'epoch': 1.0}\n",
      "  2%|▉                                           | 1/50 [00:13<08:35, 10.52s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 13.49it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "  4%|█▊                                          | 2/50 [00:19<07:44,  9.69s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 7.188381195068359, 'eval_runtime': 2.7771, 'eval_samples_per_second': 9.002, 'eval_steps_per_second': 0.36, 'epoch': 2.0}\n",
      "  4%|█▊                                          | 2/50 [00:22<07:44,  9.69s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.32it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "  6%|██▋                                         | 3/50 [00:26<06:39,  8.51s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 6.995488166809082, 'eval_runtime': 1.8856, 'eval_samples_per_second': 13.258, 'eval_steps_per_second': 0.53, 'epoch': 3.0}\n",
      "  6%|██▋                                         | 3/50 [00:28<06:39,  8.51s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 12.80it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "  8%|███▌                                        | 4/50 [00:32<05:50,  7.62s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 6.658105373382568, 'eval_runtime': 1.8599, 'eval_samples_per_second': 13.442, 'eval_steps_per_second': 0.538, 'epoch': 4.0}\n",
      "  8%|███▌                                        | 4/50 [00:34<05:50,  7.62s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.61it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 10%|████▍                                       | 5/50 [00:40<05:39,  7.55s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 6.384005546569824, 'eval_runtime': 2.7726, 'eval_samples_per_second': 9.017, 'eval_steps_per_second': 0.361, 'epoch': 5.0}\n",
      " 10%|████▍                                       | 5/50 [00:43<05:39,  7.55s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 13.88it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 12%|█████▎                                      | 6/50 [00:47<05:29,  7.49s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 6.28043270111084, 'eval_runtime': 1.8681, 'eval_samples_per_second': 13.383, 'eval_steps_per_second': 0.535, 'epoch': 6.0}\n",
      " 12%|█████▎                                      | 6/50 [00:49<05:29,  7.49s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.07it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 14%|██████▏                                     | 7/50 [00:54<05:04,  7.08s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 6.445847988128662, 'eval_runtime': 1.9295, 'eval_samples_per_second': 12.957, 'eval_steps_per_second': 0.518, 'epoch': 7.0}\n",
      " 14%|██████▏                                     | 7/50 [00:55<05:04,  7.08s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 13.41it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 16%|███████                                     | 8/50 [01:00<04:46,  6.83s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 6.2874579429626465, 'eval_runtime': 1.978, 'eval_samples_per_second': 12.639, 'eval_steps_per_second': 0.506, 'epoch': 8.0}\n",
      " 16%|███████                                     | 8/50 [01:02<04:46,  6.83s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 12.39it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 18%|███████▉                                    | 9/50 [01:06<04:33,  6.68s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 6.045103549957275, 'eval_runtime': 1.9456, 'eval_samples_per_second': 12.849, 'eval_steps_per_second': 0.514, 'epoch': 9.0}\n",
      " 18%|███████▉                                    | 9/50 [01:08<04:33,  6.68s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 12.78it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 3.3409, 'grad_norm': 1.2713483572006226, 'learning_rate': 5e-05, 'epoch': 10.0}\n",
      " 20%|████████▌                                  | 10/50 [01:13<04:24,  6.62s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 6.086159706115723, 'eval_runtime': 1.9449, 'eval_samples_per_second': 12.854, 'eval_steps_per_second': 0.514, 'epoch': 10.0}\n",
      " 20%|████████▌                                  | 10/50 [01:15<04:24,  6.62s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 12.62it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 22%|█████████▍                                 | 11/50 [01:19<04:15,  6.56s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 6.00710916519165, 'eval_runtime': 2.609, 'eval_samples_per_second': 9.582, 'eval_steps_per_second': 0.383, 'epoch': 11.0}\n",
      " 22%|█████████▍                                 | 11/50 [01:22<04:15,  6.56s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 14.60it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 24%|██████████▎                                | 12/50 [01:26<04:19,  6.82s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 6.005425930023193, 'eval_runtime': 1.969, 'eval_samples_per_second': 12.697, 'eval_steps_per_second': 0.508, 'epoch': 12.0}\n",
      " 24%|██████████▎                                | 12/50 [01:28<04:19,  6.82s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 15.42it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 26%|███████████▏                               | 13/50 [01:33<04:13,  6.86s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.9676690101623535, 'eval_runtime': 1.92, 'eval_samples_per_second': 13.021, 'eval_steps_per_second': 0.521, 'epoch': 13.0}\n",
      " 26%|███████████▏                               | 13/50 [01:35<04:13,  6.86s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 14.18it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 28%|████████████                               | 14/50 [01:40<04:01,  6.70s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.915215492248535, 'eval_runtime': 2.8339, 'eval_samples_per_second': 8.822, 'eval_steps_per_second': 0.353, 'epoch': 14.0}\n",
      " 28%|████████████                               | 14/50 [01:43<04:01,  6.70s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 14.74it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 30%|████████████▉                              | 15/50 [01:48<04:07,  7.06s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.8954877853393555, 'eval_runtime': 1.9023, 'eval_samples_per_second': 13.142, 'eval_steps_per_second': 0.526, 'epoch': 15.0}\n",
      " 30%|████████████▉                              | 15/50 [01:50<04:07,  7.06s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 14.03it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 32%|█████████████▊                             | 16/50 [01:54<03:52,  6.84s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.860997200012207, 'eval_runtime': 1.8944, 'eval_samples_per_second': 13.197, 'eval_steps_per_second': 0.528, 'epoch': 16.0}\n",
      " 32%|█████████████▊                             | 16/50 [01:56<03:52,  6.84s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 14.31it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 34%|██████████████▌                            | 17/50 [02:00<03:40,  6.69s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.819189071655273, 'eval_runtime': 1.992, 'eval_samples_per_second': 12.55, 'eval_steps_per_second': 0.502, 'epoch': 17.0}\n",
      " 34%|██████████████▌                            | 17/50 [02:02<03:40,  6.69s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 14.37it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 36%|███████████████▍                           | 18/50 [02:08<03:41,  6.91s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.9416303634643555, 'eval_runtime': 1.9949, 'eval_samples_per_second': 12.532, 'eval_steps_per_second': 0.501, 'epoch': 18.0}\n",
      " 36%|███████████████▍                           | 18/50 [02:10<03:41,  6.91s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 14.31it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 38%|████████████████▎                          | 19/50 [02:14<03:29,  6.75s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.886005878448486, 'eval_runtime': 1.9351, 'eval_samples_per_second': 12.919, 'eval_steps_per_second': 0.517, 'epoch': 19.0}\n",
      " 38%|████████████████▎                          | 19/50 [02:16<03:29,  6.75s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 14.37it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 2.9824, 'grad_norm': 1.5832058191299438, 'learning_rate': 3.7500000000000003e-05, 'epoch': 20.0}\n",
      " 40%|█████████████████▏                         | 20/50 [02:20<03:18,  6.61s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.81166934967041, 'eval_runtime': 1.9055, 'eval_samples_per_second': 13.12, 'eval_steps_per_second': 0.525, 'epoch': 20.0}\n",
      " 40%|█████████████████▏                         | 20/50 [02:22<03:18,  6.61s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 14.62it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 42%|██████████████████                         | 21/50 [02:28<03:16,  6.76s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.769496440887451, 'eval_runtime': 2.8234, 'eval_samples_per_second': 8.855, 'eval_steps_per_second': 0.354, 'epoch': 21.0}\n",
      " 42%|██████████████████                         | 21/50 [02:30<03:16,  6.76s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 14.86it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 44%|██████████████████▉                        | 22/50 [02:37<03:31,  7.56s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.851436138153076, 'eval_runtime': 2.1654, 'eval_samples_per_second': 11.545, 'eval_steps_per_second': 0.462, 'epoch': 22.0}\n",
      " 44%|██████████████████▉                        | 22/50 [02:39<03:31,  7.56s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 15.86it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 46%|███████████████████▊                       | 23/50 [02:44<03:17,  7.32s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.788053035736084, 'eval_runtime': 1.9518, 'eval_samples_per_second': 12.809, 'eval_steps_per_second': 0.512, 'epoch': 23.0}\n",
      " 46%|███████████████████▊                       | 23/50 [02:46<03:17,  7.32s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 17.64it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 48%|████████████████████▋                      | 24/50 [02:50<03:04,  7.10s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.7698187828063965, 'eval_runtime': 1.9616, 'eval_samples_per_second': 12.745, 'eval_steps_per_second': 0.51, 'epoch': 24.0}\n",
      " 48%|████████████████████▋                      | 24/50 [02:52<03:04,  7.10s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 18.29it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 50%|█████████████████████▌                     | 25/50 [02:58<03:00,  7.22s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.809445381164551, 'eval_runtime': 2.0541, 'eval_samples_per_second': 12.171, 'eval_steps_per_second': 0.487, 'epoch': 25.0}\n",
      " 50%|█████████████████████▌                     | 25/50 [03:00<03:00,  7.22s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 15.72it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 52%|██████████████████████▎                    | 26/50 [03:04<02:48,  7.02s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.684911727905273, 'eval_runtime': 1.9322, 'eval_samples_per_second': 12.938, 'eval_steps_per_second': 0.518, 'epoch': 26.0}\n",
      " 52%|██████████████████████▎                    | 26/50 [03:06<02:48,  7.02s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.55it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 54%|███████████████████████▏                   | 27/50 [03:11<02:39,  6.93s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.754876613616943, 'eval_runtime': 3.2662, 'eval_samples_per_second': 7.654, 'eval_steps_per_second': 0.306, 'epoch': 27.0}\n",
      " 54%|███████████████████████▏                   | 27/50 [03:14<02:39,  6.93s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 14.83it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 56%|████████████████████████                   | 28/50 [03:20<02:42,  7.41s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.621496200561523, 'eval_runtime': 2.8384, 'eval_samples_per_second': 8.808, 'eval_steps_per_second': 0.352, 'epoch': 28.0}\n",
      " 56%|████████████████████████                   | 28/50 [03:22<02:42,  7.41s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 15.76it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 58%|████████████████████████▉                  | 29/50 [03:27<02:37,  7.51s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.807381629943848, 'eval_runtime': 2.0902, 'eval_samples_per_second': 11.96, 'eval_steps_per_second': 0.478, 'epoch': 29.0}\n",
      " 58%|████████████████████████▉                  | 29/50 [03:29<02:37,  7.51s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.50it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 2.867, 'grad_norm': 0.5740819573402405, 'learning_rate': 2.5e-05, 'epoch': 30.0}\n",
      " 60%|█████████████████████████▊                 | 30/50 [03:34<02:24,  7.21s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.850468635559082, 'eval_runtime': 1.9832, 'eval_samples_per_second': 12.606, 'eval_steps_per_second': 0.504, 'epoch': 30.0}\n",
      " 60%|█████████████████████████▊                 | 30/50 [03:36<02:24,  7.21s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 15.32it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 62%|██████████████████████████▋                | 31/50 [03:40<02:13,  7.01s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.789525032043457, 'eval_runtime': 1.9376, 'eval_samples_per_second': 12.903, 'eval_steps_per_second': 0.516, 'epoch': 31.0}\n",
      " 62%|██████████████████████████▋                | 31/50 [03:42<02:13,  7.01s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.33it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 64%|███████████████████████████▌               | 32/50 [03:47<02:04,  6.89s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.736740589141846, 'eval_runtime': 2.0116, 'eval_samples_per_second': 12.428, 'eval_steps_per_second': 0.497, 'epoch': 32.0}\n",
      " 64%|███████████████████████████▌               | 32/50 [03:49<02:04,  6.89s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 14.41it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 66%|████████████████████████████▍              | 33/50 [03:53<01:54,  6.75s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.712686061859131, 'eval_runtime': 2.0516, 'eval_samples_per_second': 12.185, 'eval_steps_per_second': 0.487, 'epoch': 33.0}\n",
      " 66%|████████████████████████████▍              | 33/50 [03:56<01:54,  6.75s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 15.54it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 68%|█████████████████████████████▏             | 34/50 [04:00<01:46,  6.66s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.737605094909668, 'eval_runtime': 1.912, 'eval_samples_per_second': 13.075, 'eval_steps_per_second': 0.523, 'epoch': 34.0}\n",
      " 68%|█████████████████████████████▏             | 34/50 [04:02<01:46,  6.66s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 19.45it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 70%|██████████████████████████████             | 35/50 [04:06<01:38,  6.58s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.653700351715088, 'eval_runtime': 1.9597, 'eval_samples_per_second': 12.757, 'eval_steps_per_second': 0.51, 'epoch': 35.0}\n",
      " 70%|██████████████████████████████             | 35/50 [04:08<01:38,  6.58s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 13.74it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 72%|██████████████████████████████▉            | 36/50 [04:13<01:31,  6.53s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.722151279449463, 'eval_runtime': 1.9199, 'eval_samples_per_second': 13.021, 'eval_steps_per_second': 0.521, 'epoch': 36.0}\n",
      " 72%|██████████████████████████████▉            | 36/50 [04:15<01:31,  6.53s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 13.60it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 74%|███████████████████████████████▊           | 37/50 [04:19<01:23,  6.46s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.678288459777832, 'eval_runtime': 1.9569, 'eval_samples_per_second': 12.775, 'eval_steps_per_second': 0.511, 'epoch': 37.0}\n",
      " 74%|███████████████████████████████▊           | 37/50 [04:21<01:23,  6.46s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 17.37it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 76%|████████████████████████████████▋          | 38/50 [04:25<01:16,  6.41s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.672978401184082, 'eval_runtime': 1.9557, 'eval_samples_per_second': 12.783, 'eval_steps_per_second': 0.511, 'epoch': 38.0}\n",
      " 76%|████████████████████████████████▋          | 38/50 [04:27<01:16,  6.41s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 14.95it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 78%|█████████████████████████████████▌         | 39/50 [04:32<01:10,  6.43s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.671107769012451, 'eval_runtime': 1.9377, 'eval_samples_per_second': 12.902, 'eval_steps_per_second': 0.516, 'epoch': 39.0}\n",
      " 78%|█████████████████████████████████▌         | 39/50 [04:34<01:10,  6.43s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 19.54it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 2.8, 'grad_norm': 0.7035035490989685, 'learning_rate': 1.25e-05, 'epoch': 40.0}\n",
      " 80%|██████████████████████████████████▍        | 40/50 [04:38<01:04,  6.43s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.532479763031006, 'eval_runtime': 1.9409, 'eval_samples_per_second': 12.881, 'eval_steps_per_second': 0.515, 'epoch': 40.0}\n",
      " 80%|██████████████████████████████████▍        | 40/50 [04:40<01:04,  6.43s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 15.50it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 82%|███████████████████████████████████▎       | 41/50 [04:45<00:57,  6.40s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.64376163482666, 'eval_runtime': 2.3416, 'eval_samples_per_second': 10.676, 'eval_steps_per_second': 0.427, 'epoch': 41.0}\n",
      " 82%|███████████████████████████████████▎       | 41/50 [04:47<00:57,  6.40s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 18.76it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 84%|████████████████████████████████████       | 42/50 [04:51<00:52,  6.53s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.566164970397949, 'eval_runtime': 1.9153, 'eval_samples_per_second': 13.053, 'eval_steps_per_second': 0.522, 'epoch': 42.0}\n",
      " 84%|████████████████████████████████████       | 42/50 [04:53<00:52,  6.53s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.17it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 86%|████████████████████████████████████▉      | 43/50 [04:58<00:45,  6.47s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.672234535217285, 'eval_runtime': 1.9293, 'eval_samples_per_second': 12.958, 'eval_steps_per_second': 0.518, 'epoch': 43.0}\n",
      " 86%|████████████████████████████████████▉      | 43/50 [05:00<00:45,  6.47s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.83it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [05:04<00:38,  6.44s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.603124618530273, 'eval_runtime': 1.9142, 'eval_samples_per_second': 13.06, 'eval_steps_per_second': 0.522, 'epoch': 44.0}\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [05:06<00:38,  6.44s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 18.94it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [05:10<00:31,  6.37s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.640386581420898, 'eval_runtime': 1.9561, 'eval_samples_per_second': 12.78, 'eval_steps_per_second': 0.511, 'epoch': 45.0}\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [05:12<00:31,  6.37s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 15.40it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [05:17<00:25,  6.34s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.613287448883057, 'eval_runtime': 1.9342, 'eval_samples_per_second': 12.925, 'eval_steps_per_second': 0.517, 'epoch': 46.0}\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [05:18<00:25,  6.34s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 14.76it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [05:23<00:18,  6.32s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.647340774536133, 'eval_runtime': 1.9124, 'eval_samples_per_second': 13.072, 'eval_steps_per_second': 0.523, 'epoch': 47.0}\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [05:25<00:18,  6.32s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 13.27it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [05:29<00:12,  6.32s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.679899215698242, 'eval_runtime': 1.9487, 'eval_samples_per_second': 12.829, 'eval_steps_per_second': 0.513, 'epoch': 48.0}\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [05:31<00:12,  6.32s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.00it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [05:35<00:06,  6.33s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.65349006652832, 'eval_runtime': 1.9324, 'eval_samples_per_second': 12.937, 'eval_steps_per_second': 0.517, 'epoch': 49.0}\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [05:37<00:06,  6.33s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 18.26it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 2.7852, 'grad_norm': 0.6559135913848877, 'learning_rate': 0.0, 'epoch': 50.0}\n",
      "100%|███████████████████████████████████████████| 50/50 [05:42<00:00,  6.34s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.692197799682617, 'eval_runtime': 1.9472, 'eval_samples_per_second': 12.839, 'eval_steps_per_second': 0.514, 'epoch': 50.0}\n",
      "100%|███████████████████████████████████████████| 50/50 [05:44<00:00,  6.34s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 15.78it/s]\u001b[A\n",
      "                                                                                \u001b[AThere were missing keys in the checkpoint model loaded: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias'].\n",
      "{'train_runtime': 345.384, 'train_samples_per_second': 7.238, 'train_steps_per_second': 0.145, 'train_loss': 2.9550965881347655, 'epoch': 50.0}\n",
      "100%|███████████████████████████████████████████| 50/50 [05:45<00:00,  6.91s/it]\n",
      "***** train metrics *****\n",
      "  epoch                    =       50.0\n",
      "  total_flos               =   507644GF\n",
      "  train_loss               =     2.9551\n",
      "  train_runtime            = 0:05:45.38\n",
      "  train_samples_per_second =      7.238\n",
      "  train_steps_per_second   =      0.145\n"
     ]
    }
   ],
   "source": [
    "## Pretrain cehrbert using MLM\n",
    "!python3.11 -m cehrbert.runners.hf_cehrbert_pretrain_runner {ROOT_DIR}/output/cehrbert/cehrbert_pretrain_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ce8fa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the cehrbert finetuning configuration yaml file\n",
    "cehrbert_finetune_config = f\"\"\"\n",
    "#Model arguments\n",
    "model_name_or_path: \"{ROOT_DIR}/output/cehrbert/\"\n",
    "tokenizer_name_or_path: \"{ROOT_DIR}/output/cehrbert/\"\n",
    "num_hidden_layers: 6\n",
    "max_position_embeddings: 1024\n",
    "hidden_size: 768\n",
    "vocab_size: 100000\n",
    "min_frequency: 50\n",
    "include_value_prediction: false # additional CEHR-BERT learning objective\n",
    "\n",
    "#Data arguments\n",
    "cohort_folder: \"{TASK_DIR}/{TASK_NAME}/\"\n",
    "data_folder: \"{ROOT_DIR}/meds_reader/\"\n",
    "dataset_prepared_path: \"{ROOT_DIR}/output/cehrbert_dataset_prepared/\"\n",
    "\n",
    "#LORA\n",
    "use_lora: True\n",
    "lora_rank: 64\n",
    "lora_alpha: 16\n",
    "target_modules: [ \"query\", \"value\" ]\n",
    "lora_dropout: 0.1\n",
    "\n",
    "#Below is a list of Med-to-CehrBert related arguments\n",
    "preprocessing_num_workers: 2\n",
    "preprocessing_batch_size: 128\n",
    "#if is_data_in_med is false, it assumes the data is in the cehrbert format\n",
    "is_data_in_meds: true\n",
    "att_function_type: \"cehr_bert\"\n",
    "inpatient_att_function_type: \"mix\"\n",
    "include_auxiliary_token: true\n",
    "include_demographic_prompt: false\n",
    "#if the data is in the meds format, the validation split will be omitted\n",
    "#as the meds already provide train/tuning/held_out splits\n",
    "validation_split_percentage: 0.05\n",
    "\n",
    "#Huggingface Arguments\n",
    "dataloader_num_workers: 2\n",
    "dataloader_prefetch_factor: 2\n",
    "\n",
    "overwrite_output_dir: false\n",
    "resume_from_checkpoint: # automatically infer the latest checkpoint from the output folder\n",
    "seed: 42\n",
    "\n",
    "output_dir: \"{ROOT_DIR}/output/cehrbert_finetuned\"\n",
    "evaluation_strategy: \"epoch\"\n",
    "save_strategy: \"epoch\"\n",
    "eval_accumulation_steps: 10\n",
    "\n",
    "do_train: True\n",
    "do_predict: True\n",
    "\n",
    "learning_rate: 0.00005\n",
    "per_device_train_batch_size: 8\n",
    "per_device_eval_batch_size: 8\n",
    "gradient_accumulation_steps: 2\n",
    "\n",
    "num_train_epochs: 10\n",
    "warmup_steps: 10\n",
    "weight_decay: 0.01\n",
    "logging_dir: \"./logs\"\n",
    "logging_steps: 10\n",
    "\n",
    "save_total_limit:\n",
    "load_best_model_at_end: true\n",
    "metric_for_best_model: \"eval_loss\"\n",
    "greater_is_better: false\n",
    "\n",
    "report_to: \"none\"\n",
    "\"\"\"\n",
    "FINETUNE_CONFIG_FP = f\"{ROOT_DIR}/output/cehrbert/cehrbert_finetune_config.yaml\"\n",
    "with open(FINETUNE_CONFIG_FP, 'w') as f:\n",
    "    f.write(cehrbert_finetune_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf0b1d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-15 04:10:54.603602: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-15 04:10:54.603651: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-15 04:10:54.604733: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-15 04:10:54.611030: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-15 04:10:55.415341: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Directory /storage/nassim/projects/MEDS-DEV/demo/work_dir/mimiciv_demo//output/cehrbert_dataset_prepared/first_24h_meds_extension not found\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/cehrbert/runners/hf_cehrbert_finetune_runner.py\", line 116, in main\n",
      "    dataset = load_from_disk(meds_extension_path)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/datasets/load.py\", line 2630, in load_from_disk\n",
      "    raise FileNotFoundError(f\"Directory {dataset_path} not found\")\n",
      "FileNotFoundError: Directory /storage/nassim/projects/MEDS-DEV/demo/work_dir/mimiciv_demo//output/cehrbert_dataset_prepared/first_24h_meds_extension not found\n",
      "Generating train split: 36 examples [00:01, 20.55 examples/s]\n",
      "Map (num_proc=2): 100%|██████████████████| 36/36 [00:02<00:00, 15.02 examples/s]\n",
      "Generating train split: 22 examples [00:00, 34.31 examples/s]\n",
      "Map (num_proc=2): 100%|██████████████████| 22/22 [00:00<00:00, 25.79 examples/s]\n",
      "Generating train split: 16 examples [00:00, 23.03 examples/s]\n",
      "Map (num_proc=2): 100%|██████████████████| 16/16 [00:01<00:00, 15.33 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██| 36/36 [00:00<00:00, 398.28 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██| 22/22 [00:00<00:00, 585.49 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██| 16/16 [00:00<00:00, 473.62 examples/s]\n",
      "Filter (num_proc=2): 100%|██████████████| 36/36 [00:00<00:00, 204.89 examples/s]\n",
      "Filter (num_proc=2): 100%|██████████████| 22/22 [00:00<00:00, 145.32 examples/s]\n",
      "Filter (num_proc=2): 100%|██████████████| 16/16 [00:00<00:00, 107.09 examples/s]\n",
      "Map (num_proc=2): 100%|█████████████████| 36/36 [00:00<00:00, 153.21 examples/s]\n",
      "Map (num_proc=2): 100%|█████████████████| 22/22 [00:00<00:00, 108.61 examples/s]\n",
      "Map (num_proc=2): 100%|██████████████████| 16/16 [00:00<00:00, 79.96 examples/s]\n",
      "Map (num_proc=2): 100%|██████████████████| 36/36 [00:01<00:00, 21.63 examples/s]\n",
      "Map (num_proc=2): 100%|██████████████████| 22/22 [00:00<00:00, 28.46 examples/s]\n",
      "Map (num_proc=2): 100%|██████████████████| 16/16 [00:00<00:00, 16.64 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██| 36/36 [00:00<00:00, 536.98 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██| 22/22 [00:00<00:00, 933.91 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██| 16/16 [00:00<00:00, 736.32 examples/s]\n",
      "Some weights of CehrBertForClassification were not initialized from the model checkpoint at /storage/nassim/projects/MEDS-DEV/demo/work_dir/mimiciv_demo//output/cehrbert/ and are newly initialized: ['age_batch_norm.bias', 'age_batch_norm.num_batches_tracked', 'age_batch_norm.running_mean', 'age_batch_norm.running_var', 'age_batch_norm.weight', 'classifier.bias', 'classifier.weight', 'dense_layer.bias', 'dense_layer.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 10%|████▍                                       | 1/10 [00:07<01:09,  7.76s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.6832268834114075, 'eval_runtime': 0.7518, 'eval_samples_per_second': 29.263, 'eval_steps_per_second': 1.33, 'epoch': 1.0}\n",
      " 10%|████▍                                       | 1/10 [00:08<01:09,  7.76s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 58.23it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /storage/nassim/projects/MEDS-DEV/demo/work_dir/mimiciv_demo//output/cehrbert/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 20%|████████▊                                   | 2/10 [00:09<00:35,  4.48s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.6842565536499023, 'eval_runtime': 0.6771, 'eval_samples_per_second': 32.491, 'eval_steps_per_second': 1.477, 'epoch': 2.0}\n",
      " 20%|████████▊                                   | 2/10 [00:10<00:35,  4.48s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 57.03it/s]\u001b[A\n",
      "                                                                                \u001b[A/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /storage/nassim/projects/MEDS-DEV/demo/work_dir/mimiciv_demo//output/cehrbert/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'train_runtime': 10.7437, 'train_samples_per_second': 33.508, 'train_steps_per_second': 0.931, 'train_loss': 0.35596993565559387, 'epoch': 2.0}\n",
      " 20%|████████▊                                   | 2/10 [00:10<00:42,  5.37s/it]\n",
      "/storage/nassim/miniconda3/envs/cehrbert/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /storage/nassim/projects/MEDS-DEV/demo/work_dir/mimiciv_demo//output/cehrbert/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  total_flos               =    15105GF\n",
      "  train_loss               =      0.356\n",
      "  train_runtime            = 0:00:10.74\n",
      "  train_samples_per_second =     33.508\n",
      "  train_steps_per_second   =      0.931\n",
      "Some weights of CehrBertForClassification were not initialized from the model checkpoint at /storage/nassim/projects/MEDS-DEV/demo/work_dir/mimiciv_demo//output/cehrbert/ and are newly initialized: ['age_batch_norm.bias', 'age_batch_norm.num_batches_tracked', 'age_batch_norm.running_mean', 'age_batch_norm.running_var', 'age_batch_norm.weight', 'classifier.bias', 'classifier.weight', 'dense_layer.bias', 'dense_layer.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Predicting: 100%|█████████████████████████████████| 2/2 [00:00<00:00,  3.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# ## Finetune cehrbert for the downstream task\n",
    "!python3.11 -m cehrbert.runners.hf_cehrbert_finetune_runner {ROOT_DIR}/output/cehrbert/cehrbert_finetune_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9977fc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"roc_auc\": 0.4,\n",
      "    \"pr_auc\": 0.05,\n",
      "    \"test_loss\": 0.6830134987831116\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat {ROOT_DIR}/output/cehrbert_finetuned/test_results.json"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "cehrbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
